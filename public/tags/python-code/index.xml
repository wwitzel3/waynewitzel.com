<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
      <title>Python Code on occasional posts about technology </title>
      <generator uri="https://gohugo.io">Hugo</generator>
    <link>http://localhost:1313/tags/python-code/index.xml</link>
    <language>en-us</language>
    
    
    <updated>Mon, 01 Jan 0001 00:00:00 UTC</updated>
    
    <item>
      <title>Code: Buffer CGI file uploads in Windows</title>
      <link>http://localhost:1313/buffer-cgi-uploads/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 UTC</pubDate>
      
      <guid>http://localhost:1313/buffer-cgi-uploads/</guid>
      <description>

&lt;p&gt;Note to self, when handling CGI file uploads on a Windows machine, you need the following boiler plate to properly handler binary files.
[sourcecode language=&amp;lsquo;python&amp;rsquo;]
try: # Windows needs stdio set for binary mode.
    import msvcrt
    msvcrt.setmode (0, os.O_BINARY) # stdin  = 0
    msvcrt.setmode (1, os.O_BINARY) # stdout = 1
except ImportError:
    pass
[/sourcecode]
Also, if you&amp;rsquo;re handling very large files and don&amp;rsquo;t want to eat up all your memory saving them using the copyfileobj method, you can use a generator to buffer read and write the file.&lt;/p&gt;

&lt;p&gt;[sourcecode language=&amp;lsquo;python&amp;rsquo;]
def buffer(f, sz=1024):
    while True:
        chunk = f.read(sz)
        if not chunk: break
        yield chunk&lt;/p&gt;

&lt;h1 id=&#34;toc_0&#34;&gt;then use it like this &amp;hellip;&lt;/h1&gt;

&lt;p&gt;for chunk in buffer(fp.file)
[/sourcecode]&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Code: Saving in memory file to disk</title>
      <link>http://localhost:1313/memory-to-file/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 UTC</pubDate>
      
      <guid>http://localhost:1313/memory-to-file/</guid>
      <description>

&lt;p&gt;Okay, I discovered this today when looking to increase the speed at which uploaded documents were saved to disk. Now, I can&amp;rsquo;t explain the inner workings of why it is fast(er), all I know is that with the exact same form upload test ran 100 times with a 25MB file over a 100Mbit/s network this method was on average a whole 2.3 seconds faster over traditional methods of write, writelines, etc..&lt;/p&gt;

&lt;p&gt;How does this extend to real-world production usage over external networks, well no idea. Though I plan to find out. So you all will be the first to know as soon as I find some guinea pig site that does enough file uploads to implement this on.
[sourcecode language=&amp;lsquo;python&amp;rsquo;]&lt;/p&gt;

&lt;h1 id=&#34;toc_0&#34;&gt;Minus some boiler plate for validity and variable setup.&lt;/h1&gt;

&lt;p&gt;import os
import shutil
memory_file = request.POST[&amp;lsquo;upload&amp;rsquo;]
disk_file = open(os.path.join(save_folder, save_name),&amp;lsquo;w&amp;rsquo;)
shutil.copyfileobj(memory_file.file, disk_file)
disk_file.close()
[/sourcecode]&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>